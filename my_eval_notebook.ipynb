{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified copy of legalbench/static-eval/static-eval/eval_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from langchain.llms import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from task_utils import TASKS, load_data, load_prompt, generate_prompts\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import PromptTemplate, HuggingFaceHub, HuggingFacePipeline, LLMChain\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai-api-secret-key.txt', 'r') as f:\n",
    "    openai_api_key = f.read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('huggingface-hub-api-token-write.txt', 'r') as f:\n",
    "    hf_api_key = f.read().strip()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, interface):\n",
    "    output = ''\n",
    "    if interface == 'openai':\n",
    "        GPT_TURBO = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=600)\n",
    "        output = GPT_TURBO([HumanMessage(content=prompt)]).content\n",
    "    elif interface == 'huggingface':\n",
    "        prompt = PromptTemplate.from_template(template=prompt)\n",
    "        llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\":1e-10}, task=\"text2text-generation\")\n",
    "        llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        output = llm_chain.run(prompt=prompt)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks: list, tasks_dir: str, interface='openai'):\n",
    "    report = dict()\n",
    "    for task in tqdm(tasks):\n",
    "        train_df, test_df = load_data(task=task, tasks_dir=tasks_dir)\n",
    "        prompt_template = load_prompt(prompt_name=\"base_prompt.txt\", task=task, tasks_dir=tasks_dir)\n",
    "        prompts = generate_prompts(prompt_template=prompt_template, data_df=train_df)\n",
    "        report[task] = dict()\n",
    "        targets = list()\n",
    "        outputs = list()\n",
    "        for prompt, data in zip(prompts, train_df.iterrows()):\n",
    "            datapoint_id, data = data\n",
    "            output = call_llm(prompt, interface)\n",
    "            output = output.strip()\n",
    "            targets.append(data['answer'])\n",
    "            outputs.append(output)\n",
    "            success = output == data['answer']\n",
    "            report[task][datapoint_id] = {\n",
    "                'prompt': prompt,\n",
    "                'generated_output': output,\n",
    "                'correct_output': data['answer'],\n",
    "                'success': output == data['answer']\n",
    "            }\n",
    "        report[task]['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(targets, outputs)\n",
    "    \n",
    "    print('Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [03:22<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.5131578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tasks_dir = 'legalbench/static-eval/legalbench/'\n",
    "cuad_tasks = [task for task in TASKS if task.startswith('cuad')]\n",
    "report = evaluate(cuad_tasks, tasks_dir, 'huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cuad_flant5base_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_dir = 'legalbench/static-eval/legalbench/'\n",
    "\n",
    "# warnings are to be expected\n",
    "\n",
    "report = evaluate(tasks=random.sample(TASKS, 10), tasks_dir=tasks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-llm-hack",
   "language": "python",
   "name": "legal-llm-hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
