{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified copy of legalbench/static-eval/static-eval/eval_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import baseten\n",
    "import os\n",
    "import json\n",
    "import sklearn.metrics\n",
    "import random\n",
    "from langchain.llms import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from task_utils import TASKS, load_data, load_prompt, generate_prompts\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import PromptTemplate, HuggingFaceHub, HuggingFacePipeline, LLMChain\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai-api-secret-key.txt', 'r') as f:\n",
    "    openai_api_key = f.read().strip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('huggingface-hub-api-token-write.txt', 'r') as f:\n",
    "    hf_api_key = f.read().strip()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m API key set.\n",
      "INFO:baseten:API key set.\n"
     ]
    }
   ],
   "source": [
    "with open('baseten-api-key.txt', 'r') as f:\n",
    "    baseten_api_key = f.read().strip()\n",
    "os.environ[\"MY_BASETEN_API_KEY\"] = baseten_api_key\n",
    "baseten.login(api_key=baseten_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, interface):\n",
    "    output = ''\n",
    "    if interface == 'openai':\n",
    "        GPT_TURBO = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=600)\n",
    "        output = GPT_TURBO([HumanMessage(content=prompt)]).content\n",
    "    elif interface == 'huggingface':\n",
    "        prompt = PromptTemplate.from_template(template=prompt)\n",
    "        llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\":0}, task=\"text2text-generation\")\n",
    "        llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        output = llm_chain.run(prompt=prompt)\n",
    "    elif interface == 'baseten':\n",
    "        model = baseten.deployed_model_id('1Bbyz9q')  # MPT-7B-base\n",
    "        request = {\n",
    "            \"prompt\" : prompt,\n",
    "            \"temperature\": 0.01,\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "        output = model.predict(request)\n",
    "        if output and output['status'] == 'success' and output['data']:\n",
    "            output = output['data'].splitlines()[0]\n",
    "        else:\n",
    "            output = ''\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks: list, tasks_dir: str, interface='openai'):\n",
    "    report = dict()\n",
    "    for task in tqdm(tasks):\n",
    "        train_df, test_df = load_data(task=task, tasks_dir=tasks_dir)\n",
    "        prompt_template = load_prompt(prompt_name=\"base_prompt.txt\", task=task, tasks_dir=tasks_dir)\n",
    "        prompts = generate_prompts(prompt_template=prompt_template, data_df=train_df)\n",
    "        report[task] = dict()\n",
    "        targets = list()\n",
    "        outputs = list()\n",
    "        for prompt, data in zip(prompts, train_df.iterrows()):\n",
    "            datapoint_id, data = data\n",
    "            output = call_llm(prompt, interface)\n",
    "            output = output.strip()\n",
    "            targets.append(data['answer'])\n",
    "            outputs.append(output)\n",
    "            success = output == data['answer']\n",
    "            report[task][datapoint_id] = {\n",
    "                'prompt': prompt,\n",
    "                'generated_output': output,\n",
    "                'correct_output': data['answer'],\n",
    "                'success': output == data['answer']\n",
    "            }\n",
    "        report[task]['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(targets, outputs)\n",
    "    print('Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_dir = 'legalbench/static-eval/legalbench/'\n",
    "cuad_tasks = [task for task in TASKS if task.startswith('cuad')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(cuad_tasks, tasks_dir, 'huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cuad_flant5base_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 38/38 [40:54<00:00, 64.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.48245614035087714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "report = evaluate(cuad_tasks, tasks_dir, 'baseten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cuad_mpt7bbase_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_count = 0\n",
    "for prompt in report:\n",
    "    for i in range(len(report[prompt])-1):\n",
    "        if report[prompt][i]['generated_output'] == 'Yes':\n",
    "            yes_count += 1\n",
    "yes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate(tasks=random.sample(TASKS, 10), tasks_dir=tasks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-llm-hack",
   "language": "python",
   "name": "legal-llm-hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
